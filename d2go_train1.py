# -*- coding: utf-8 -*-
"""d2go_train1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oIMr4waqHOcsZtzcULHRW9dWl1tjlqML
"""

!lscpu

# install dependencies: and torch and torchvision
!pip install pyyaml==6.0
import torch , torchvision
from google.colab.patches import cv2_imshow
print(torch.__version__, torch.cuda.is_available())
print(torchvision.__version__)

# install detectron2: (Colab has CUDA 10.1 + torch 1.7)
# See https://detectron2.readthedocs.io/tutorials/install.html for instructions
import torch 
assert torch.__version__.startswith("1.9")
!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'

#mobilevision
!python -m pip install 'git+https://github.com/facebookresearch/mobile-vision.git'

#d2go
!python3 -m pip install 'git+https://github.com/facebookresearch/d2go.git'

#exit(0)

"""# **select the model **

[model zoo](https://github.com/facebookresearch/d2go/blob/master/MODEL_ZOO.md).

[model Tree](https://github.com/facebookresearch/d2go/tree/master/configs)
"""

#pip install google-colab
from google.colab import drive
drive.mount('/content/drive')

# if your dataset is in COCO format, this cell can be replaced by the following three lines:
from detectron2.data.datasets import register_coco_instances

register_coco_instances("my_dataset_train", {}, "/content/drive/MyDrive/d2go/gineua_pig/gineuapig_train.json", "/content/drive/MyDrive/d2go/gineua_pig/gineua_pig_train")
register_coco_instances("my_dataset_val", {}, "/content/drive/MyDrive/d2go/gineua_pig/gineuapig_test.json", "/content/drive/MyDrive/d2go/gineua_pig/gineua_pig_test")


#register_coco_instances("my_dataset_train2", {}, "/content/drive/MyDrive/d2go/fast_output.json", "/content/drive/MyDrive/d2go/fast rcnn image")
#register_coco_instances("my_dataset_val2", {}, "/content/drive/MyDrive/d2go/fast_output.json", "/content/drive/MyDrive/d2go/fast rcnn image")

from detectron2.data import MetadataCatalog ,DatasetCatalog

sample_metadata = MetadataCatalog.get("my_dataset_train")
dataset_dicts = DatasetCatalog.get("my_dataset_train")
print(sample_metadata)
print(dataset_dicts)

#from d2go.model_zoo import model_zoo

"""
        "faster_rcnn_fbnetv3a_C4.yaml": "246823121/model_0479999.pth",
        "faster_rcnn_fbnetv3a_dsmask_C4.yaml": "250414811/model_0399999.pth",
        "faster_rcnn_fbnetv3g_fpn.yaml": "250356938/model_0374999.pth",
        "mask_rcnn_fbnetv3a_C4.yaml": "250355374/model_0479999.pth",
        "mask_rcnn_fbnetv3a_dsmask_C4.yaml": "250414867/model_0399999.pth",
        "mask_rcnn_fbnetv3g_fpn.yaml": "250376154/model_0404999.pth",
        "keypoint_rcnn_fbnetv3a_dsmask_C4.yaml": "250430934/model_0389999.pth",
    
"""

import os
from d2go.runner import Detectron2GoRunner
from d2go.runner import GeneralizedRCNNRunner
from d2go.model_zoo import model_zoo
torch.backends.quantized.engine = 'qnnpack'

def prepare_for_launch():

    #runner = Detectron2GoRunner()
    runner = GeneralizedRCNNRunner()
    cfg = runner.get_default_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("mask_rcnn_fbnetv3a_C4.yaml"))
    cfg.MODEL_EMA.ENABLED = False
    cfg.DATASETS.TRAIN = ("my_dataset_train",)
    cfg.DATASETS.TEST = ("my_dataset_val",)
    cfg.DATALOADER.NUM_WORKERS = 2 #no of cpu to be used
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("mask_rcnn_fbnetv3a_C4.yaml")  # Let training initialize from model zoo
    cfg.SOLVER.IMS_PER_BATCH = 2
    cfg.INPUT.CROP.ENABLED = True
    cfg.QUANTIZATION.BACKEND = 'qnnpack'
    cfg.INPUT.MAX_SIZE_TEST = 160
    cfg.INPUT.MIN_SIZE_TEST = 112
    #cfg.INPUT.RANDOM_FLIP = 'horizontal'
    #cfg.INPUT.RANDOM_FLIP = 'vertical'
    cfg.SOLVER.BASE_LR = 0.003  # pick a good LR
    cfg.SOLVER.MAX_ITER = 20   # you will need to train longer for a practical dataset
    cfg.SOLVER.STEPS = []        # do not decay learning rate
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8   # set the testing threshold for this model
    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 40   # faster, and good enough for this toy dataset (default: 512)
    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (mahesh). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
    # NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.
    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
    return cfg, runner

cfg, runner = prepare_for_launch()
model = runner.build_model(cfg)
runner.do_train(cfg, model, resume=False)

"""Precision measures how accurate is your predictions. i.e. the percentage of your predictions are correct.

Recall measures how good you find all the positives. For example, we can find 80% of the possible positive cases in our top K predictions.
"""

#put this in config files
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "/content/output/model_final.pth")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set the testing threshold for this model
cfg.DATASETS.TRAIN = ("my_dataset_train", )
cfg.DATASETS.TEST = ("my_dataset_val", )

from detectron2.engine import DefaultPredictor
from d2go.utils.demo_predictor import DemoPredictor
from PIL import Image
import cv2
from detectron2.utils import visualizer 
from detectron2.utils.visualizer import ColorMode
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from matplotlib import pyplot as plt



#im =cv2.imread('/content/seperateimages/20210612113500.jpg')

im =cv2.imread(r"/content/drive/MyDrive/d2go/persons.jpg")
#Open image using Image module
#im = Image.open(r"/content/drive/MyDrive/d2go/persons.jpg")

#model = runner.build_model(cfg)
#predictor = DefaultPredictor(cfg)


model = runner.build_model(cfg)
#model = create_predictor(predictor_path)
predictor = DemoPredictor(model)

#model = create_predictor(predictor_path)
#predictor = DemoPredictor(model)


outputs = predictor(im)
# the output object categories and corresponding bounding boxes
print(outputs["instances"].pred_classes)
v = Visualizer(im[:, :, ::-1], MetadataCatalog.get("my_dataset_train"))
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
plt.imshow(out.get_image()[:, :, ::-1])

f = open('config1.yml', 'w')
f.write(cfg.dump())
f.close()

#prediction with model
import detectron2
import cv2
import os
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.utils.video_visualizer import VideoVisualizer
from detectron2.data import MetadataCatalog
from detectron2.utils.visualizer import ColorMode
from d2go.runner import Detectron2GoRunner
from matplotlib import pyplot as plt
#from google.colab.patches import cv2_imshow
from d2go.utils.demo_predictor import DemoPredictor
from mobile_cv.predictor.api import create_predictor
from d2go.runner import GeneralizedRCNNRunner
import time


runner = GeneralizedRCNNRunner()
cfg = runner.get_default_cfg()

#cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_C4_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_C4_1x.yaml")

#cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml")

cfg.merge_from_file(r"/content/config1.yml")
cfg.MODEL.WEIGHTS =  os.path.join("/content/output/model_final.pth")
cfg.MODEL.DEVICE = "cuda"

#cfg.merge_from_file(model_zoo.get_config_file("COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml")

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6  # set threshold for this model

model = runner.build_model(cfg)
predictor = DefaultPredictor(cfg)


#model = runner.build_model(cfg)
#model = create_predictor(predictor_path)
#predictor = DemoPredictor(model)

#model = create_predictor(predictor_path)
#predictor = DemoPredictor(model)

#cap=cv2.VideoCapture('/content/video.mp4')
cap=cv2.VideoCapture("/content/drive/MyDrive/d2go/VID_20210701_130909.mp4")
fps = cap.get(cv2.CAP_PROP_FPS)
num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

frame_width = int(cap.get(3))
frame_height = int(cap.get(4))

#out = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (640,480))
#out = cv2.VideoWriter('output3.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))
#out = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc(*'XVID'), 30.0, (frame_width,frame_height))
out = cv2.VideoWriter('output_mod.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (frame_width,frame_height))

"""out = cv2.VideoWriter(
                'newvideo1.mkv',
                # some installation of opencv may not support x264 (due to its license),
                # you can try other format (e.g. MPEG)
                fourcc=cv2.VideoWriter_fourcc(*"x264"),
                #fourcc=cv2.VideoWriter_fourcc('M','J','P','G'), 
                fps=float(frames_per_second),
                frameSize=(width, height),
                isColor=True,
            )"""
start = time.time()
while (cap.isOpened()):
	
    ret,frame=cap.read(0)
    print(fps)
    print(num_frames)
    
    try:
      outputs = predictor(frame)
      #v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]))
      v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), instance_mode=ColorMode.IMAGE_BW)
      v = v.draw_instance_predictions(frame, outputs["instances"].to('cpu'))
      #cv2_imshow("window", v.get_image())
      
      out.write(v.get_image())
      
      print(outputs["instances"].pred_boxes)
      print(outputs["instances"].pred_classes)
      omt = str(outputs["instances"].pred_classes)
      outpred = omt[8:9]
      print(outpred)
      end = time.time()
      timeis = start -end
      print("timeis",timeis)
      start = end
         
    except:
      break
    
cap.release()
out.release()
#cv2.destroyAllWindows()

datasets = cfg.DATASETS.TRAIN[0]
print(datasets)



#convert to int8 model for raspberry pi
import copy
from detectron2.data import build_detection_test_loader
from d2go.export.api import convert_and_export_predictor
from d2go.export.d2_meta_arch import patch_d2_meta_arch
from d2go.runner import GeneralizedRCNNRunner
from d2go.runner import Detectron2GoRunner
import os
import logging


# disable all the warnings
previous_level = logging.root.manager.disable
logging.disable(logging.INFO)

patch_d2_meta_arch()

# https://github.com/pytorch/pytorch/issues/29327#issue-518778762
torch.backends.quantized.engine = 'qnnpack'
cfg.merge_from_file(r"/content/config1.yml")
cfg.QUANTIZATION.BACKEND = 'qnnpack'
runner = GeneralizedRCNNRunner()
model = runner.build_model(cfg, eval_only=True)
model.cpu()

datasets = cfg.DATASETS.TRAIN[0]

cfg.QUANTIZATION.BACKEND = 'qnnpack'
data_loader = runner.build_detection_test_loader(cfg, datasets)


predictor_path = convert_and_export_predictor(
  copy.deepcopy(cfg),
  copy.deepcopy(model),
  "torchscript_int8@tracing",
  './',
  data_loader
)

# recover the logging level
logging.disable(previous_level)

f = open('config2.yml', 'w')
f.write(cfg.dump())
f.close()

#prediction with int8 model
import detectron2
import cv2
import os
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.utils.video_visualizer import VideoVisualizer
from detectron2.data import MetadataCatalog
from detectron2.utils.visualizer import ColorMode
from d2go.runner import Detectron2GoRunner
from matplotlib import pyplot as plt
#from google.colab.patches import cv2_imshow
from d2go.utils.demo_predictor import DemoPredictor
from mobile_cv.predictor.api import create_predictor
from d2go.runner import GeneralizedRCNNRunner
import time


runner = GeneralizedRCNNRunner()
cfg = runner.get_default_cfg()

#cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_C4_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_C4_1x.yaml")

#cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml")

cfg.merge_from_file(r"/content/config2.yml")
cfg.MODEL.WEIGHTS =  os.path.join("/content/torchscript_int8@tracing/data.pth")
cfg.MODEL.DEVICE = "cuda"

#cfg.merge_from_file(model_zoo.get_config_file("COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml")

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6  # set threshold for this model

#model = runner.build_model(cfg)
#predictor = DefaultPredictor(cfg)

start = time.time()
#model = runner.build_model(cfg)
model = create_predictor(predictor_path)
predictor = DemoPredictor(model)

#model = create_predictor(predictor_path)
#predictor = DemoPredictor(model)

#cap=cv2.VideoCapture('/content/video.mp4')
cap=cv2.VideoCapture("/content/drive/MyDrive/d2go/VID_20210701_130909.mp4")
fps = cap.get(cv2.CAP_PROP_FPS)
num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

frame_width = int(cap.get(3))
frame_height = int(cap.get(4))

#out = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (640,480))
#out = cv2.VideoWriter('output3.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))
#out = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc(*'XVID'), 30.0, (frame_width,frame_height))
out = cv2.VideoWriter('output_int8.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (frame_width,frame_height))

"""out = cv2.VideoWriter(
                'newvideo1.mkv',
                # some installation of opencv may not support x264 (due to its license),
                # you can try other format (e.g. MPEG)
                fourcc=cv2.VideoWriter_fourcc(*"x264"),
                #fourcc=cv2.VideoWriter_fourcc('M','J','P','G'), 
                fps=float(frames_per_second),
                frameSize=(width, height),
                isColor=True,
            )"""

while (cap.isOpened()):
	
    ret,frame=cap.read(0)
    print(fps)
    print(num_frames)
    
    try:
      outputs = predictor(frame)
      #v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]))
      v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), instance_mode=ColorMode.IMAGE_BW)
      v = v.draw_instance_predictions(frame, outputs["instances"].to('cpu'))
      #cv2_imshow("window", v.get_image())
      
      out.write(v.get_image())
      
      print(outputs["instances"].pred_boxes)
      print(outputs["instances"].pred_classes)
      omt = str(outputs["instances"].pred_classes)
      outpred = omt[8:9]
      print(outpred)
      end = time.time()
      print("timeis",end - start)
      start = end
     
    
    except:
      break
    
cap.release()
out.release()
#cv2.destroyAllWindows()



#for android
from typing import List, Dict
import torch

class Wrapper(torch.nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model
        coco_idx_list = [1]

        self.coco_idx = torch.tensor(coco_idx_list)

    def forward(self, inputs: List[torch.Tensor]):
        x = inputs[0].unsqueeze(0) * 255
        scale = 320.0 / min(x.shape[-2], x.shape[-1])
        x = torch.nn.functional.interpolate(x, scale_factor=scale, mode="bilinear", align_corners=True, recompute_scale_factor=True)
        out = self.model(x[0])
        res : Dict[str, torch.Tensor] = {}
        res["boxes"] = out[0] / scale
        res["labels"] = torch.index_select(self.coco_idx, 0, out[1])
        res["scores"] = out[2]
        return inputs, [res]

orig_model = torch.jit.load(os.path.join(predictor_path, "model.jit"))
wrapped_model = Wrapper(orig_model)
scripted_model = torch.jit.script(wrapped_model)
scripted_model.save("d2go.pt")

metrics = runner.do_test(cfg, model)

print(metrics)



#@****************************************************************************************************************************

#prediction with int8 model
import detectron2
import cv2
import os
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.utils.video_visualizer import VideoVisualizer
from detectron2.data import MetadataCatalog
from detectron2.utils.visualizer import ColorMode
from d2go.runner import Detectron2GoRunner
from matplotlib import pyplot as plt
#from google.colab.patches import cv2_imshow
from d2go.utils.demo_predictor import DemoPredictor
from mobile_cv.predictor.api import create_predictor
from d2go.runner import GeneralizedRCNNRunner



runner = GeneralizedRCNNRunner()
cfg = runner.get_default_cfg()

#cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_C4_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_C4_1x.yaml")

#cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml")

cfg.merge_from_file(r"/content/config2.yml")
cfg.MODEL.WEIGHTS =  os.path.join("/content/torchscript_int8@tracing/data.pth")
cfg.MODEL.DEVICE = "cuda"

#cfg.merge_from_file(model_zoo.get_config_file("COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml"))
#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Keypoints/keypoint_rcnn_R_50_FPN_1x.yaml")

cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6  # set threshold for this model

#model = runner.build_model(cfg)
#predictor = DefaultPredictor(cfg)


#model = runner.build_model(cfg)
model = create_predictor(predictor_path)
predictor = DemoPredictor(model)

#model = create_predictor(predictor_path)
#predictor = DemoPredictor(model)

#cap=cv2.VideoCapture('/content/video.mp4')
cap=cv2.VideoCapture("/content/drive/MyDrive/d2go/VID_20210701_130909.mp4")
fps = cap.get(cv2.CAP_PROP_FPS)
num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

frame_width = int(cap.get(3))
frame_height = int(cap.get(4))

#out = cv2.VideoWriter('output.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (640,480))
#out = cv2.VideoWriter('output3.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))
#out = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc(*'XVID'), 30.0, (frame_width,frame_height))
out = cv2.VideoWriter('output_int8.mp4', cv2.VideoWriter_fourcc(*'mp4v'), 30.0, (frame_width,frame_height))

"""out = cv2.VideoWriter(
                'newvideo1.mkv',
                # some installation of opencv may not support x264 (due to its license),
                # you can try other format (e.g. MPEG)
                fourcc=cv2.VideoWriter_fourcc(*"x264"),
                #fourcc=cv2.VideoWriter_fourcc('M','J','P','G'), 
                fps=float(frames_per_second),
                frameSize=(width, height),
                isColor=True,
            )"""

while (cap.isOpened()):
	
    ret,frame=cap.read(0)
    print(fps)
    print(num_frames)
    
    try:
      outputs = predictor(frame)
      #v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]))
      v = VideoVisualizer(MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), instance_mode=ColorMode.IMAGE_BW)
      v = v.draw_instance_predictions(frame, outputs["instances"].to('cpu'))
      #cv2_imshow("window", v.get_image())
      
      out.write(v.get_image())
      
      print(outputs["instances"].pred_boxes)
      print(outputs["instances"].pred_classes)
      omt = str(outputs["instances"].pred_classes)
      outpred = omt[8:9]
      print(outpred)
     
    
    except:
      break
    
cap.release()
out.release()
#cv2.destroyAllWindows()

/usr/local/lib/python3.7/dist-packages/mobile_cv/mobile_cv/common/tests/test_misc.py

from mobile_cv.common













